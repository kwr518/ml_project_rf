# ======================================================
# 04_random_forest_tuning.py
# RandomForest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ======================================================

import sqlite3
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# =========================
# ì„¤ì •
# =========================
DB_NAME = "water_quality_full.db"
TABLE_NAME = "water_quality"

FEATURES = [
    'HR', 'RE', 'NON', 'BRO', 'AL', 'CF', 'SO',
    'TU',   # íƒë„
    'RC'    # ì”ë¥˜ì—¼ì†Œ
]
TARGET = "PH"

RANDOM_STATE = 42
MODEL_PATH = "rf_ph_model_tuned1.pkl"

# =========================
# 1. DB ë¡œë“œ
# =========================
conn = sqlite3.connect(DB_NAME)
df = pd.read_sql(f"SELECT * FROM {TABLE_NAME}", conn)
conn.close()

print("ì´ ë°ì´í„° ìˆ˜:", len(df))

# =========================
# 2. ì „ì²˜ë¦¬
# =========================
df = df.replace({
    "ë¶ˆê²€ì¶œ": 0,
    "ê²€ì¶œ": 1,
    "ì í•©": 1,
    "ë¶€ì í•©": 0,
    "ì¼ë°˜ì„¸ê· ": 1
})

for col in FEATURES + [TARGET]:
    df[col] = pd.to_numeric(df[col], errors="coerce")

df = df.dropna(subset=FEATURES + [TARGET])

print("ëª¨ë¸ë§ ë°ì´í„° ìˆ˜:", len(df))

# =========================
# 3. Train / Test ë¶„ë¦¬
# =========================
X = df[FEATURES]
y = df[TARGET]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=RANDOM_STATE
)

# =========================
# 4. íŠœë‹ íŒŒë¼ë¯¸í„° ê³µê°„
# =========================
param_dist = {
    "n_estimators": [300, 600, 1000],
    "max_depth": [None, 8, 12, 16, 20],
    "min_samples_leaf": [1, 2, 5, 10],
    "min_samples_split": [2, 5, 10],
    "max_features": ["sqrt", 0.5, 0.8]
}

base_model = RandomForestRegressor(
    random_state=RANDOM_STATE,
    n_jobs=-1
)

# =========================
# 5. RandomizedSearchCV
# =========================
search = RandomizedSearchCV(
    estimator=base_model,
    param_distributions=param_dist,
    n_iter=10,   # ì‹œê°„ ì—¬ìœ  ì—†ìœ¼ë©´ 15
    cv=3,
    scoring="neg_root_mean_squared_error",
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

print("\n í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘...")
search.fit(X_train, y_train)

best_model = search.best_estimator_

print("\n ìµœì  íŒŒë¼ë¯¸í„°")
for k, v in search.best_params_.items():
    print(f"{k}: {v}")

# =========================
# 6. ì„±ëŠ¥ í‰ê°€ (íŠœë‹ ëª¨ë¸)
# =========================
y_pred = best_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n íŠœë‹ëœ RandomForest ì„±ëŠ¥")
print(f"RMSE : {rmse:.4f}")
print(f"RÂ²   : {r2:.4f}")

# =========================
# 7. Feature Importance
# =========================
importances = pd.Series(
    best_model.feature_importances_,
    index=FEATURES
).sort_values(ascending=False)

print("\nğŸ” Feature Importance (Tuned)")
print(importances)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(8, 5))
importances.plot(kind="bar", color="steelblue")
plt.title("Tuned RandomForest Feature Importance (pH ì˜ˆì¸¡)")
plt.ylabel("Importance")
plt.xlabel("ìˆ˜ì§ˆ í•­ëª©")
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.show()
# =========================
# 8. ëª¨ë¸ ì €ì¥
# =========================
joblib.dump(
    {
        "model": best_model,
        "features": FEATURES,
        "target": TARGET
    },
    MODEL_PATH
)

print(f"\n íŠœë‹ ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {MODEL_PATH}")

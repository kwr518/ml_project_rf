# ======================================================
# 03_random_forest_regression.py
# RandomForest íšŒê·€ë¡œ pH ì˜ˆì¸¡ + ëª¨ë¸ ì €ì¥
# ======================================================

import sqlite3
import pandas as pd
import numpy as np
import joblib

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# =========================
# ì„¤ì •
# =========================
DB_NAME = "water_quality_full.db"
TABLE_NAME = "water_quality"

#  ê³µì • ê´€ì  ë°˜ì˜í•œ ìµœì¢… ë³€ìˆ˜
FEATURES = [
    'HR',   # ê²½ë„
    'RE',   # ì¦ë°œì”ë¥˜ë¬¼
    'NON',  # ì§ˆì‚°ì„±ì§ˆì†Œ
    'BRO',  # ë¸Œë¡¬ì‚°ì—¼
    'AL',   # ì•Œë£¨ë¯¸ëŠ„
    'CF',   # í´ë¡œë¡œí¬ë¦„
    'SO',   # í™©ì‚°ì´ì˜¨
    'TU',   # íƒë„ (ì‘ì§‘ íŒë‹¨)
    'RC'    # ì”ë¥˜ì—¼ì†Œ (ì†Œë… ìš´ì „ í•µì‹¬)
]

TARGET = 'PH'
MODEL_PATH = "rf_ph_model.pkl"
RANDOM_STATE = 42

# =========================
# 1. DB â†’ DataFrame
# =========================
conn = sqlite3.connect(DB_NAME)
df = pd.read_sql(f"SELECT * FROM {TABLE_NAME}", conn)
conn.close()

print("ì´ ë°ì´í„° ìˆ˜:", len(df))

# =========================
# 2. ì „ì²˜ë¦¬
# =========================
# ë¬¸ìì—´ â†’ ìˆ˜ì¹˜ ì¹˜í™˜
df = df.replace({
    "ë¶ˆê²€ì¶œ": 0,
    "ê²€ì¶œ": 1,
    "ì í•©": 1,
    "ë¶€ì í•©": 0,
    "ì¼ë°˜ì„¸ê· ": 1
})

# ìˆ«ì ë³€í™˜
for col in FEATURES + [TARGET]:
    df[col] = pd.to_numeric(df[col], errors="coerce")

# ê²°ì¸¡ì¹˜ ì œê±°
df = df.dropna(subset=FEATURES + [TARGET])

print("ëª¨ë¸ í•™ìŠµ ë°ì´í„° ìˆ˜:", len(df))

# =========================
# 3. í•™ìŠµ / í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
# =========================
X = df[FEATURES]
y = df[TARGET]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=RANDOM_STATE
)

# =========================
# 4. RandomForest í•™ìŠµ
# =========================
rf = RandomForestRegressor(
    n_estimators=300,
    min_samples_leaf=5,
    random_state=RANDOM_STATE,
    n_jobs=-1
)

rf.fit(X_train, y_train)

# =========================
# 5. ì˜ˆì¸¡
# =========================
y_pred = rf.predict(X_test)

# =========================
# 6. ì„±ëŠ¥ í‰ê°€
# =========================
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n RandomForest íšŒê·€ ì„±ëŠ¥")
print(f"RMSE : {rmse:.4f}")
print(f"MAE  : {mae:.4f}")
print(f"RÂ²   : {r2:.4f}")

# =========================
# 7. Feature Importance
# =========================
importances = pd.Series(
    rf.feature_importances_,
    index=FEATURES
).sort_values(ascending=False)

print("\nğŸ” Feature Importance")
print(importances)

# =========================
# 8. ëª¨ë¸ ì €ì¥
# =========================
joblib.dump({
    "model": rf,
    "features": FEATURES,
    "target": TARGET
}, MODEL_PATH)

print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {MODEL_PATH}")

# =========================
# 9. ì¤‘ìš”ë„ ì‹œê°í™”
# =========================
plt.figure(figsize=(9, 5))
importances.plot(kind="bar")
plt.title("RandomForest Feature Importance (pH ì˜ˆì¸¡)")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()
